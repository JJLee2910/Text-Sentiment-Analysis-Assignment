{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TXTA Assignment \n",
    "Q1. Form tokenization and Filter stop words & punctuation (12 marks)\n",
    "1.\tDemonstrate word tokenisation using the split function, Regular Expression and NLTK packages separately and report the output. (4 marks) (code)\n",
    "2.\tJustify the most suitable tokenisation operation for text analytics. Support your answer using obtained outputs. (2 marks) (doc)\n",
    "3.\tDemonstrate stop words and punctuations removal and report the output suitably along with the stop words found in the given text corpus. (4 marks) (code)\n",
    "4.\tExplain the importance of filtering the stop words and punctuations in text analytics. (2 marks) (doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Textual information in the world can be broadly categorized into two main types: facts and opinions. Facts are objective expressions about entities, events, and their properties. Opinions are usually subjective expressions that describe people’s sentiments, appraisals, or feelings toward entities, events, and their properties.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open('Assignment Data\\\\Data_1.txt')\n",
    "text = file.read()\n",
    "text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokenization using Split: \n",
      "['Textual', 'information', 'in', 'the', 'world', 'can', 'be', 'broadly', 'categorized', 'into', 'two', 'main', 'types:', 'facts', 'and', 'opinions.', 'Facts', 'are', 'objective', 'expressions', 'about', 'entities,', 'events,', 'and', 'their', 'properties.', 'Opinions', 'are', 'usually', 'subjective', 'expressions', 'that', 'describe', 'people’s', 'sentiments,', 'appraisals,', 'or', 'feelings', 'toward', 'entities,', 'events,', 'and', 'their', 'properties.'] \n",
      "\n",
      "Word Tokenization using RegEx: \n",
      "['Textual', 'information', 'in', 'the', 'world', 'can', 'be', 'broadly', 'categorized', 'into', 'two', 'main', 'types:', 'facts', 'and', 'opinions.', 'Facts', 'are', 'objective', 'expressions', 'about', 'entities,', 'events,', 'and', 'their', 'properties.', 'Opinions', 'are', 'usually', 'subjective', 'expressions', 'that', 'describe', 'people’s', 'sentiments,', 'appraisals,', 'or', 'feelings', 'toward', 'entities,', 'events,', 'and', 'their', 'properties.'] \n",
      "\n",
      "Word Tokenization using NLTK: \n",
      "['Textual', 'information', 'in', 'the', 'world', 'can', 'be', 'broadly', 'categorized', 'into', 'two', 'main', 'types', ':', 'facts', 'and', 'opinions', '.', 'Facts', 'are', 'objective', 'expressions', 'about', 'entities', ',', 'events', ',', 'and', 'their', 'properties', '.', 'Opinions', 'are', 'usually', 'subjective', 'expressions', 'that', 'describe', 'people', '’', 's', 'sentiments', ',', 'appraisals', ',', 'or', 'feelings', 'toward', 'entities', ',', 'events', ',', 'and', 'their', 'properties', '.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#1. Demonstrate word tokenisation using the split function, Regular Expression and NLTK packages separately and report the output.\n",
    "\n",
    "# split function \n",
    "a = text.split()\n",
    "print(\"Word Tokenization using Split: \")\n",
    "print(a, '\\n')\n",
    "\n",
    "# Regular Expression\n",
    "regex = re.split(\" \", text)\n",
    "print(\"Word Tokenization using RegEx: \")\n",
    "print(regex, '\\n')\n",
    "\n",
    "# NLTK\n",
    "tokens = word_tokenize(text) \n",
    "print(\"Word Tokenization using NLTK: \")\n",
    "print(tokens) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changes made on RE, if u use \".split\" function, u might not splitted the punctuations out correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split function output:\n",
      "['Textual', 'information', 'in', 'the', 'world', 'can', 'be', 'broadly', 'categorized', 'into', 'two', 'main', 'types:', 'facts', 'and', 'opinions.', 'Facts', 'are', 'objective', 'expressions', 'about', 'entities,', 'events,', 'and', 'their', 'properties.', 'Opinions', 'are', 'usually', 'subjective', 'expressions', 'that', 'describe', 'people’s', 'sentiments,', 'appraisals,', 'or', 'feelings', 'toward', 'entities,', 'events,', 'and', 'their', 'properties.']\n",
      "\n",
      "Regular Expression output:\n",
      "['Textual', 'information', 'in', 'the', 'world', 'can', 'be', 'broadly', 'categorized', 'into', 'two', 'main', 'types', 'facts', 'and', 'opinions', 'Facts', 'are', 'objective', 'expressions', 'about', 'entities', 'events', 'and', 'their', 'properties', 'Opinions', 'are', 'usually', 'subjective', 'expressions', 'that', 'describe', 'people', 's', 'sentiments', 'appraisals', 'or', 'feelings', 'toward', 'entities', 'events', 'and', 'their', 'properties']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jangj\\anaconda3\\envs\\JJ\\lib\\site-packages\\scipy\\__init__.py:177: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NLTK output:\n",
      "['Textual', 'information', 'in', 'the', 'world', 'can', 'be', 'broadly', 'categorized', 'into', 'two', 'main', 'types', ':', 'facts', 'and', 'opinions', '.', 'Facts', 'are', 'objective', 'expressions', 'about', 'entities', ',', 'events', ',', 'and', 'their', 'properties', '.', 'Opinions', 'are', 'usually', 'subjective', 'expressions', 'that', 'describe', 'people', '’', 's', 'sentiments', ',', 'appraisals', ',', 'or', 'feelings', 'toward', 'entities', ',', 'events', ',', 'and', 'their', 'properties', '.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Read the data from the file\n",
    "with open(\"Assignment Data\\\\Data_1.txt\", \"r\") as file:\n",
    "    data = file.read()\n",
    "\n",
    "# Using split function for tokenization\n",
    "split_tokens = data.split()\n",
    "print(\"Split function output:\")\n",
    "print(split_tokens)\n",
    "\n",
    "# Using Regular Expression for tokenization\n",
    "regex_tokens = re.findall(r'\\b\\w+\\b', data) # use \\b to detect start and end of the word, then \\w+ is use to check 1 or more letter or number.\n",
    "print(\"\\nRegular Expression output:\")\n",
    "print(regex_tokens)\n",
    "\n",
    "# Using NLTK for tokenization\n",
    "nltk_tokens = word_tokenize(data)\n",
    "print(\"\\nNLTK output:\")\n",
    "print(nltk_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Join sentence: \n",
      "textual information world can broadly categorized two main types facts opinions facts objective expressions about entities events properties opinions usually subjective expressions describe sentiments appraisals feelings toward entities events properties\n"
     ]
    }
   ],
   "source": [
    "#3. Demonstrate stop words and punctuations removal and report the output suitably along with the stop words found in the given text corpus.\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def remove_stopwords_punctuation(txt):\n",
    "\n",
    "    # derive stop words\n",
    "    stop_words = set(['a', 'an', 'and', 'are', 'as', 'at', 'be', 'but', 'by', 'for', 'if', 'in', 'into', 'is', 'it', 'no', 'not', 'of', 'on', 'or', 'such', 'that', 'the', 'their', 'then', 'there', 'these', 'they', 'this', 'to', 'was', 'will', 'with'])\n",
    "\n",
    "    # tokenize the text\n",
    "    ab = word_tokenize(txt)\n",
    "\n",
    "    # removing stop words and punctuations\n",
    "    filtered_words = [word.lower() for word in ab if word.lower() not in stop_words and word.isalpha()]\n",
    "\n",
    "\n",
    "    # Join back into a sentence\n",
    "    fresult = ' '.join(filtered_words)\n",
    "\n",
    "    return fresult\n",
    "\n",
    "fresultnew = remove_stopwords_punctuation(text)\n",
    "print(\"Join sentence: \")\n",
    "print(fresultnew)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
