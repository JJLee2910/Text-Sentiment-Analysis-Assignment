{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TXTA Assignment \n",
    "Q1. Form tokenization and Filter stop words & punctuation (12 marks)\n",
    "1.\tDemonstrate word tokenisation using the split function, Regular Expression and NLTK packages separately and report the output. (4 marks) (code)\n",
    "2.\tJustify the most suitable tokenisation operation for text analytics. Support your answer using obtained outputs. (2 marks) (doc)\n",
    "3.\tDemonstrate stop words and punctuations removal and report the output suitably along with the stop words found in the given text corpus. (4 marks) (code)\n",
    "4.\tExplain the importance of filtering the stop words and punctuations in text analytics. (2 marks) (doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Textual information in the world can be broadly categorized into two main types: facts and opinions. Facts are objective expressions about entities, events, and their properties. Opinions are usually subjective expressions that describe people's sentiments, appraisals, or feelings toward entities, events, and their properties.\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open('Assignment Data\\\\Data_1.txt')\n",
    "text = file.read()\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokenization using Split: \n",
      "['Textual', 'information', 'in', 'the', 'world', 'can', 'be', 'broadly', 'categorized', 'into', 'two', 'main', 'types:', 'facts', 'and', 'opinions.', 'Facts', 'are', 'objective', 'expressions', 'about', 'entities,', 'events,', 'and', 'their', 'properties.', 'Opinions', 'are', 'usually', 'subjective', 'expressions', 'that', 'describe', 'people’s', 'sentiments,', 'appraisals,', 'or', 'feelings', 'toward', 'entities,', 'events,', 'and', 'their', 'properties.'] \n",
      "\n",
      "Word Tokenization using RegEx: \n",
      "['Textual', 'information', 'in', 'the', 'world', 'can', 'be', 'broadly', 'categorized', 'into', 'two', 'main', 'types:', 'facts', 'and', 'opinions.', 'Facts', 'are', 'objective', 'expressions', 'about', 'entities,', 'events,', 'and', 'their', 'properties.', 'Opinions', 'are', 'usually', 'subjective', 'expressions', 'that', 'describe', 'people’s', 'sentiments,', 'appraisals,', 'or', 'feelings', 'toward', 'entities,', 'events,', 'and', 'their', 'properties.'] \n",
      "\n",
      "Word Tokenization using NLTK: \n",
      "['Textual', 'information', 'in', 'the', 'world', 'can', 'be', 'broadly', 'categorized', 'into', 'two', 'main', 'types', ':', 'facts', 'and', 'opinions', '.', 'Facts', 'are', 'objective', 'expressions', 'about', 'entities', ',', 'events', ',', 'and', 'their', 'properties', '.', 'Opinions', 'are', 'usually', 'subjective', 'expressions', 'that', 'describe', 'people', '’', 's', 'sentiments', ',', 'appraisals', ',', 'or', 'feelings', 'toward', 'entities', ',', 'events', ',', 'and', 'their', 'properties', '.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#1. Demonstrate word tokenisation using the split function, Regular Expression and NLTK packages separately and report the output.\n",
    "\n",
    "# split function \n",
    "a = text.split()\n",
    "print(\"Word Tokenization using Split: \")\n",
    "print(a, '\\n')\n",
    "\n",
    "# Regular Expression\n",
    "regex = re.split(\" \", text)\n",
    "print(\"Word Tokenization using RegEx: \")\n",
    "print(regex, '\\n')\n",
    "\n",
    "# NLTK\n",
    "tokens = word_tokenize(text) \n",
    "print(\"Word Tokenization using NLTK: \")\n",
    "print(tokens) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changes made on RE, if u use \".split\" function, u might not splitted the punctuations out correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split function output:\n",
      "['Textual', 'information', 'in', 'the', 'world', 'can', 'be', 'broadly', 'categorized', 'into', 'two', 'main', 'types:', 'facts', 'and', 'opinions.', 'Facts', 'are', 'objective', 'expressions', 'about', 'entities,', 'events,', 'and', 'their', 'properties.', 'Opinions', 'are', 'usually', 'subjective', 'expressions', 'that', 'describe', 'people’s', 'sentiments,', 'appraisals,', 'or', 'feelings', 'toward', 'entities,', 'events,', 'and', 'their', 'properties.']\n",
      "\n",
      "Regular Expression output:\n",
      "['Textual', 'information', 'in', 'the', 'world', 'can', 'be', 'broadly', 'categorized', 'into', 'two', 'main', 'types', 'facts', 'and', 'opinions', 'Facts', 'are', 'objective', 'expressions', 'about', 'entities', 'events', 'and', 'their', 'properties', 'Opinions', 'are', 'usually', 'subjective', 'expressions', 'that', 'describe', 'people', 's', 'sentiments', 'appraisals', 'or', 'feelings', 'toward', 'entities', 'events', 'and', 'their', 'properties']\n",
      "\n",
      "NLTK output:\n",
      "['Textual', 'information', 'in', 'the', 'world', 'can', 'be', 'broadly', 'categorized', 'into', 'two', 'main', 'types', ':', 'facts', 'and', 'opinions', '.', 'Facts', 'are', 'objective', 'expressions', 'about', 'entities', ',', 'events', ',', 'and', 'their', 'properties', '.', 'Opinions', 'are', 'usually', 'subjective', 'expressions', 'that', 'describe', 'people', '’', 's', 'sentiments', ',', 'appraisals', ',', 'or', 'feelings', 'toward', 'entities', ',', 'events', ',', 'and', 'their', 'properties', '.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Read the data from the file\n",
    "with open(\"Assignment Data\\\\Data_1.txt\", \"r\") as file:\n",
    "    data = file.read()\n",
    "\n",
    "# Using split function for tokenization\n",
    "split_tokens = data.split()\n",
    "print(\"Split function output:\")\n",
    "print(split_tokens)\n",
    "\n",
    "# Using Regular Expression for tokenization\n",
    "regex_tokens = re.findall(r'\\b\\w+\\b', data) # use \\b to detect start and end of the word, then \\w+ is use to check 1 or more letter or number.\n",
    "print(\"\\nRegular Expression output:\")\n",
    "print(regex_tokens)\n",
    "\n",
    "# Using NLTK for tokenization\n",
    "nltk_tokens = word_tokenize(data)\n",
    "print(\"\\nNLTK output:\")\n",
    "print(nltk_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Join sentence:  textual information world broadly categorized two main types facts opinions facts objective expressions entities events properties opinions usually subjective expressions describe people sentiments appraisals feelings toward entities events properties\n"
     ]
    }
   ],
   "source": [
    "#3. Demonstrate stop words and punctuations removal and report the output suitably along with the stop words found in the given text corpus.\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def remove_stopwords_punctuation(txt):\n",
    "\n",
    "    # derive stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # tokenize the text\n",
    "    ab = word_tokenize(txt)\n",
    "\n",
    "    # removing stop words and punctuations\n",
    "    filtered_words = [word.lower() for word in ab if word.lower() not in stop_words and word.isalpha()]\n",
    "\n",
    "\n",
    "    # Join back into a sentence\n",
    "    fresult = ' '.join(filtered_words)\n",
    "\n",
    "    return fresult\n",
    "\n",
    "fresultnew = remove_stopwords_punctuation(text)\n",
    "print(\"Join sentence: \", fresultnew)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of manually typing the stop words, can use library to help us to classified out the stop words and print them out if u want to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Tokens after Stop Words and Punctuation Removal:\n",
      "['Textual', 'information', 'world', 'broadly', 'categorized', 'two', 'main', 'types', 'facts', 'opinions', 'Facts', 'objective', 'expressions', 'entities', 'events', 'properties', 'Opinions', 'usually', 'subjective', 'expressions', 'describe', 'people', '’', 'sentiments', 'appraisals', 'feelings', 'toward', 'entities', 'events', 'properties']\n",
      "\n",
      "Stop Words Found in the Text:\n",
      "{'above', 'your', 'were', 'then', \"it's\", 'but', 'himself', 'they', 'more', \"didn't\", 'ourselves', 'while', 'some', 't', 'y', 'herself', 'both', 'when', 'what', 've', 'aren', 'any', 'the', 'am', 'as', 'down', 'myself', 'this', 'again', \"needn't\", 'he', 'don', 'won', 'll', 'an', 'once', 'off', \"that'll\", 'nor', 'out', 'here', \"isn't\", 'hasn', 'those', 'which', 'theirs', 'most', 'my', 'same', 're', 'yourselves', 'isn', 'who', 'other', 'below', 'where', 'its', \"haven't\", 'if', 'than', 'will', 'our', 'we', \"mightn't\", 'to', 'too', 'was', \"shan't\", \"wasn't\", 'she', 'did', 'i', 'by', 'yourself', 'how', \"she's\", 'had', 'own', 'of', 'for', 'should', 'from', 'them', 'been', 'haven', 'me', 'against', 'shouldn', \"you'd\", 'being', 'her', 'themselves', 'doing', 'and', 'can', 'didn', 'is', 'few', 'such', 'wouldn', 'no', 'before', 'm', 'ours', \"don't\", 'until', 'whom', 'shan', 'his', 'needn', 'it', \"aren't\", 'doesn', 'does', 'why', 'over', \"you're\", 'now', 'hadn', \"shouldn't\", 'not', 'mightn', 'him', 'into', 'so', \"hasn't\", 'are', 'has', 'these', 'after', 'o', 'or', 'under', \"mustn't\", 'hers', 'between', 'yours', 'only', 'd', 'with', 'be', \"should've\", 'ma', 's', 'mustn', 'have', 'just', \"won't\", 'weren', \"you've\", \"you'll\", 'wasn', 'at', 'up', 'further', 'having', 'about', 'ain', \"couldn't\", 'couldn', \"wouldn't\", 'in', 'because', 'do', 'during', \"weren't\", 'each', 'through', 'their', 'there', 'all', 'a', 'that', 'itself', 'very', 'you', \"hadn't\", 'on', \"doesn't\"}\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(data)\n",
    "\n",
    "# Remove punctuation\n",
    "tokens_no_punct = [word for word in tokens if word not in string.punctuation]\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens_no_punct if word.lower() not in stop_words]\n",
    "\n",
    "# Print the filtered tokens and stop words\n",
    "print(\"Filtered Tokens after Stop Words and Punctuation Removal:\")\n",
    "print(filtered_tokens)\n",
    "\n",
    "print(\"\\nStop Words Found in the Text:\")\n",
    "print(stop_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
